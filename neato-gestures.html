<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neato Gesture Control - Carlo Colizzi</title>
    <link rel="stylesheet" href="assets/css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link href="https://fonts.cdnfonts.com/css/roobert" rel="stylesheet">
</head>
<body>
    <!-- Header placeholder -->
    <div id="header-placeholder"></div>

    <main class="case-study-page">
        <!-- Hero Section -->
        <section class="case-study-hero">
            <div class="container">
                <div class="case-study-intro">
                    <div class="case-study-meta">
                        <span class="case-study-year">2022</span>
                        <span class="case-study-category">Computer Vision & Human-Robot Interaction</span>
                    </div>
                    <h1>Neato Gesture Commands</h1>
                    <p class="case-study-description">An intuitive gesture-based control system for Neato robots, using MediaPipe for hand detection and machine learning for custom gesture recognition. This project enables natural human-robot interaction through hand movements, allowing users to control robot navigation and behaviors without physical interfaces.</p>
                    <div class="github-container">
                        <a href="https://github.com/carlocolizzi/neato-gestures" target="_blank" class="github-button">
                            <i class="fab fa-github"></i>
                            View Code
                        </a>
                    </div>
                </div>
            </div>
        </section>

        <!-- Overview Section -->
        <section class="case-study-overview">
            <div class="container">
                <div class="overview-grid">
                    <div class="overview-item">
                        <h3>Controls</h3>
                        <div class="stat">8+</div>
                        <p>Distinct gesture commands</p>
                    </div>
                    <div class="overview-item">
                        <h3>Framework</h3>
                        <div class="stat">ROS2</div>
                        <p>Robot Operating System integration</p>
                    </div>
                    <div class="overview-item">
                        <h3>Recognition</h3>
                        <div class="stat">Custom</div>
                        <p>Trainable gesture models</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Challenge Section -->
        <section class="case-study-challenge">
            <div class="container">
                <div class="section-grid">
                    <div class="section-content">
                        <h2>The Challenge</h2>
                        <div class="challenge-content">
                            <p>Creating a reliable and intuitive gesture-based control system presented several technical challenges:</p>
                            <ul>
                                <li>Implementing robust hand detection that works reliably in varying conditions</li>
                                <li>Developing a system for training custom gesture recognition rather than relying on pre-trained models</li>
                                <li>Creating intuitive mappings between gestures and robot behaviors</li>
                                <li>Ensuring real-time processing with minimal latency for responsive control</li>
                                <li>Designing advanced features like path drawing that translate hand movements into robot trajectories</li>
                                <li>Integrating gesture recognition with ROS2 for reliable robot control</li>
                            </ul>
                        </div>
                    </div>
                    <div class="section-image">
                        <img src="assets/images/gestures-3.gif" alt="Gesture Recognition System">
                    </div>
                </div>
            </div>
        </section>

        <!-- Solution Section -->
        <section class="case-study-solution">
            <div class="container">
                <div class="section-grid reverse">
                    <div class="section-content">
                        <h2>The Solution</h2>
                        <div class="solution-content">
                            <p>We developed a comprehensive gesture recognition system using advanced computer vision techniques:</p>
                            <ul>
                                <li>Implemented MediaPipe for accurate hand detection and tracking of key points (fingertips, knuckles)</li>
                                <li>Created a custom training system for recognizing user-defined gestures</li>
                                <li>Developed intuitive number-based gesture commands for basic robot control</li>
                                <li>Implemented specific behavior algorithms triggered by different gestures</li>
                                <li>Built a prototype path drawing feature that tracks finger movements to create custom robot paths</li>
                            </ul>
                        </div>
                    </div>
                    <div class="section-image">
                        <img src="assets/images/gestures-1.gif" alt="Hand Tracking System">
                        <p class="image-caption">MediaPipe hand tracking visualization</p>
                    </div>
                </div>
                <div class="mechanical-content">
                    <p>Our approach to gesture control focused on user-friendliness and system extensibility:</p>
                    <ul>
                        <li>Used MediaPipe's hand landmark detection to identify 21 key points on each hand</li>
                        <li>Implemented custom gesture training where users could create their own gesture dataset</li>
                        <li>Designed a speed control system that uses the distance between thumb and index finger as a variable control</li>
                        <li>Created an integrated system where the robot responds to gestures in real-time via ROS2 communication</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Implementation Section -->
        <section class="case-study-implementation">
            <div class="container">
                <div class="section-grid">
                    <div class="section-content">
                        <h2>Gesture Recognition System</h2>
                        <div class="implementation-content">
                            <p>Our implementation went through several distinct development phases:</p>
                            <p><strong>Tool Selection:</strong> After evaluating various computer vision libraries, we chose MediaPipe for its robust hand detection capabilities. MediaPipe provided pre-built functionalities for identifying and tracking hands, including detailed finger position data that was essential for our gesture recognition system.</p>
                            <p><strong>Custom Gesture Training:</strong> Rather than using pre-trained models limited to standard gestures, we implemented a system that allowed us to create and train recognition for custom gestures. This involved capturing images of our gestures in real-time and using them to build a custom dataset for machine learning-based recognition.</p>
                            <p><strong>Behavior Integration:</strong> We mapped specific gestures to robot behaviors, initially using random gesture assignments before transitioning to a more intuitive number-based system for better usability.</p>
                        </div>
                    </div>
                    <div class="section-image">
                        <img src="assets/images/gestures-2.gif" alt="Number Gesture System">
                        <p class="image-caption">Number-based gesture control system</p>
                    </div>
                </div>
                <div class="section-grid">
                    <div class="section-content">
                        <h2>Robot Control & Advanced Features</h2>
                        <div class="implementation-content">
                            <p>Our gesture control system evolved from basic commands to more sophisticated interactions:</p>
                            <p><strong>Movement Control:</strong> We initially implemented basic movement controls using the robot's odometry to create consistent patterns. Rather than using time-based commands, we leveraged the robot's positioning system to achieve precise movements and turns for driving in various shapes.</p>
                            <p><strong>Number Gesture System:</strong> We redesigned our gesture controls to use intuitive number-based commands, with each number triggering a specific robot behavior:</p>
                            <ul>
                                <li>Number 1 (pointer finger): Forward movement</li>
                                <li>Number 2 (peace sign): Turn right</li>
                                <li>Number 3 (three fingers): Turn left</li>
                                <li>Number 4 (four fingers): Stop</li>
                                <li>Number 0 (fist): Continuous spinning</li>
                                <li>Open hand: Draw a square</li>
                                <li>Triangle gesture: Draw a triangle</li>
                                <li>Thumb and index finger pinch: Speed control (distance proportional to speed)</li>
                            </ul>
                            <p><strong>Path Drawing Prototype:</strong> We began work on an advanced feature that would allow users to "draw" paths in the air for the robot to follow. This involved tracking finger movement, recognizing the intended shape, and converting it to precise robot navigation commands.</p>
                        </div>
                    </div>
                    <div class="section-image">
                        <img src="assets/images/gestures-4.gif" alt="Path Drawing Feature">
                        <p class="image-caption">Path drawing prototype visualization</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Future Development Section -->
        <section class="case-study-users">
            <div class="container">
                <h2>Future Development</h2>
                <div class="users-content">
                    <div class="user-group">
                        <h3>Path Drawing Enhancement</h3>
                        <ul>
                            <li>Waypoint-based drawing system to create more precise paths</li>
                            <li>Recognition of pauses in finger movement to mark corner points</li>
                            <li>Support for multiple path geometries (lines, zigzags, shapes)</li>
                            <li>Translation of detected paths into precise robot movement commands</li>
                            <li>Improved curve handling and path smoothing algorithms</li>
                        </ul>
                    </div>
                    <div class="user-group">
                        <h3>System Improvements</h3>
                        <ul>
                            <li>Enhanced gesture recognition accuracy in varied lighting conditions</li>
                            <li>Implementation of more sophisticated gesture combinations</li>
                            <li>User-specific gesture calibration and preferences</li>
                            <li>Integration with other robot capabilities (mapping, object interaction)</li>
                            <li>Expansion of the gesture vocabulary for more complex commands</li>
                        </ul>
                    </div>
                </div>
                <div class="users-content">
                    <div class="user-group">
                        <h3>Results & Achievements</h3>
                        <ul>
                            <li>Successfully implemented a working gesture control system for the Neato robot</li>
                            <li>Achieved reliable recognition of multiple distinct hand gestures</li>
                            <li>Created intuitive mappings between gestures and robot behaviors</li>
                            <li>Developed a functional prototype for custom gesture training</li>
                            <li>Established a foundation for future development of path drawing functionality</li>
                        </ul>
                    </div>
                    <div class="user-group">
                        <h3>Technical Learnings</h3>
                        <ul>
                            <li>Computer vision techniques for hand detection and tracking</li>
                            <li>Machine learning approaches for custom gesture recognition</li>
                            <li>ROS2 integration for translating gesture commands to robot actions</li>
                            <li>Real-time processing optimization for responsive control</li>
                            <li>Human-robot interaction design principles for intuitive interfaces</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Next Project -->
        <section class="next-project">
            <div class="container">
                <a href="neato-particle-filter.html" class="next-project-link">
                    <span>Next Project</span>
                    <h3>Neato Particle Filter â†’</h3>
                </a>
            </div>
        </section>
    </main>

    <!-- Footer placeholder -->
    <div id="footer-placeholder"></div>

    <script>
        // Load header
        fetch('components/header.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('header-placeholder').innerHTML = data;
            });

        // Load footer
        fetch('components/footer.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('footer-placeholder').innerHTML = data;
            });
    </script>
    <script src="assets/js/main.js"></script>
</body>
</html>